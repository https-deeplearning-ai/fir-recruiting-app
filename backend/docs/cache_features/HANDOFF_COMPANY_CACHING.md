# Company Caching Implementation - Handoff Document

**Date:** November 13, 2025
**Status:** üü° Partially Complete - Database Schema Blocking Full Functionality
**Priority:** üî¥ CRITICAL - Must fix to enable caching

---

## Executive Summary

The company caching system is **95% complete** and all code changes are working correctly. However, **a database schema constraint is preventing cache saves**, making the cache unable to accumulate data across searches.

**Impact:** Every search starts fresh (0% cache hit rate) instead of building up cached companies for future searches.

**Fix Required:** Single SQL command to remove NOT NULL constraint on `website` column.

**Estimated Time:** 2 minutes

---

## Current State

### ‚úÖ What's Working

1. **Two-tier caching architecture** - Implemented and functional
   - Tier 1: `company_lookup_cache` (name‚ÜíID mappings)
   - Tier 2: `stored_companies` (full company data)

2. **Code changes completed:**
   - ‚úÖ `save_stored_company()` signature updated with `collected_at` and `collection_method` parameters
   - ‚úÖ `discover_companies()` returns `cache_metrics` tuple
   - ‚úÖ `research_companies_for_jd()` unpacks `cache_metrics` correctly
   - ‚úÖ Reverse enrichment implemented (ID‚Üíname mapping after collect)
   - ‚úÖ API pricing corrected ($0.10 search, $0.20 collect)

3. **Cache retrieval working:**
   - Cache lookups execute correctly
   - Cache hits are detected and logged
   - 4-tier fallback lookup works for cache misses

4. **Pipeline completion:**
   - All phases complete successfully (discovery ‚Üí enrichment ‚Üí screening ‚Üí employee sampling)
   - No crashes or Python exceptions
   - Cache metrics calculated and returned correctly

### ‚ùå What's Broken

**CRITICAL ISSUE: Cache Saves Failing**

**Error:**
```
[CACHE] ‚ùå Failed to save Paytm Business: 400
{"code":"23502", "details":"null value in column \"website\" of relation \"company_lookup_cache\" violates not-null constraint"}
```

**Impact:**
- 95% of cache save operations fail (65/68 companies in last test)
- Cache cannot accumulate data
- Every search starts with 0% cache hit rate
- No cost savings realized
- Wasted API credits on repeated lookups

**Root Cause:**
- Database table `company_lookup_cache` has `website` column with NOT NULL constraint
- Web search discovery finds company names but NOT websites
- When code tries to save: `"website": None` ‚Üí database rejects with constraint violation
- Error is caught and logged but doesn't crash pipeline

---

## The Fix (CRITICAL - Must Do First)

### Step 1: Fix Database Schema

**Run this SQL in Supabase SQL Editor:**

```sql
-- Remove NOT NULL constraint from website column
ALTER TABLE company_lookup_cache
ALTER COLUMN website DROP NOT NULL;

-- Verify the change
SELECT column_name, is_nullable
FROM information_schema.columns
WHERE table_name = 'company_lookup_cache'
  AND column_name = 'website';

-- Expected result: is_nullable = 'YES'
```

**Why this fixes it:**
- Allows saving companies discovered via web search (which don't have websites)
- Website field can be populated later if/when we get full company data
- Most companies ARE found via name-based search, not website

---

### Step 2: Verify Database Columns Exist

**Check these columns were added:**

```sql
SELECT column_name, data_type, column_default
FROM information_schema.columns
WHERE table_name = 'stored_companies'
  AND column_name IN ('collection_method', 'collected_at', 'data_source')
ORDER BY column_name;
```

**Expected result:**
- `collection_method` TEXT DEFAULT 'collect'
- `collected_at` TIMESTAMP
- `data_source` TEXT DEFAULT 'coresignal_company_base'

**If missing, add them:**

```sql
ALTER TABLE stored_companies
ADD COLUMN IF NOT EXISTS collection_method TEXT DEFAULT 'collect',
ADD COLUMN IF NOT EXISTS collected_at TIMESTAMP,
ADD COLUMN IF NOT EXISTS data_source TEXT DEFAULT 'coresignal_company_base';
```

---

## Testing After Fix

### Test 1: First Search (Cache Population)

**Steps:**
1. Clear old sessions:
   ```sql
   UPDATE company_research_sessions
   SET status = 'failed'
   WHERE status = 'running'
     AND created_at < NOW() - INTERVAL '1 hour';
   ```

2. In UI, run company research with query:
   ```
   payment processing and payment gateway companies
   ```

3. Wait for completion (2-3 minutes)

**Expected Logs:**
```bash
üîç Looking up CoreSignal company IDs for ~70 companies...

# Cache misses (first time)
   ‚äó [CACHE MISS] Stripe - executing 4-tier lookup...
      ‚úÖ Found: ID=12345678 (tier 1, website)
   [CACHE] ‚úÖ Saved Stripe ‚Üí 12345678  ‚Üê Should see SUCCESS, not FAIL

üíæ Cache Performance:
   Cache Hits: 0 (0.0% hit rate)
   Cache Misses: 70
   üí∞ Credits Saved: 0 (~$0.00 at $0.10/credit)
```

**Verify in Database:**
```sql
SELECT company_name, company_id, website, lookup_successful
FROM company_lookup_cache
WHERE created_at > NOW() - INTERVAL '10 minutes'
ORDER BY created_at DESC
LIMIT 10;
```

**Expected:** 70 rows inserted, many with `website = NULL` (this should now be allowed)

---

### Test 2: Second Search (Cache Hits)

**Steps:**
1. Run the **SAME search query** again immediately
2. Should complete in <30 seconds

**Expected Logs:**
```bash
üîç Looking up CoreSignal company IDs for ~70 companies...

# Cache HITS (second time)
   ‚úÖ [CACHE HIT] Stripe: ID=12345678 (confidence: 1.0)
   ‚úÖ [CACHE HIT] PayPal: ID=87654321 (confidence: 1.0)
   ‚úÖ [CACHE HIT] Square: ID=99999999 (confidence: 1.0)

üíæ Cache Performance:
   Cache Hits: 70 (100.0% hit rate)  ‚Üê SHOULD BE 100%!
   Cache Misses: 0
   üí∞ Credits Saved: 70 (~$7.00 at $0.10/credit)  ‚Üê COST SAVINGS!
```

**Success Criteria:**
- ‚úÖ Cache hit rate: 100%
- ‚úÖ Credits saved: ~$7
- ‚úÖ Search completes in <30 seconds (vs 2-3 minutes first time)

---

## Additional Improvements (Optional - After Critical Fix)

### Improvement 1: Better Error Logging

**File:** `backend/company_id_cache_service.py` line 158

**Current:**
```python
else:
    print(f"[CACHE] ‚ùå Failed to save {company_name}: {response.status_code} {response.text}")
```

**Improved:**
```python
else:
    error_detail = response.json() if response.text else {}
    error_msg = error_detail.get('message', response.text)
    print(f"[CACHE] ‚ùå Failed to save {company_name}: {response.status_code}")
    print(f"         Error: {error_msg}")
    if 'violates not-null constraint' in str(error_detail):
        print(f"         üí° TIP: Run 'ALTER TABLE company_lookup_cache ALTER COLUMN website DROP NOT NULL' in Supabase")
```

**Why:** Helps diagnose schema issues faster

---

### Improvement 2: Track Cache Save Failures

**File:** `backend/company_research_service.py` line 1210

**Add counter:**
```python
# After line 1214
cache_save_failures = 0
```

**Track failures (line 1338):**
```python
# In the except block after save_to_cache
except Exception as e:
    print(f"      [CACHE] Error saving to cache: {e}")
    cache_errors += 1
    cache_save_failures += 1  # NEW
```

**Include in metrics (line 1387):**
```python
cache_metrics = {
    "cache_hits": cache_hits,
    "cache_misses": cache_misses,
    "cache_errors": cache_errors,
    "cache_save_failures": cache_save_failures,  # NEW
    "cache_save_success_rate": ((cache_misses - cache_save_failures) / cache_misses * 100) if cache_misses > 0 else 0,  # NEW
    "cache_hit_rate": cache_hit_rate,
    "credits_saved": credits_saved,
    "estimated_cost_saved": credits_saved * 0.10
}
```

**Why:** Better visibility into cache health

---

### Improvement 3: Session Status Cleanup

**File:** `backend/company_research_service.py` line 140

**Add cleanup at start:**
```python
# At the beginning of research_companies_for_jd()
# Clear any stale "failed" status from previous runs
await self._update_session_status(jd_id, "running", {
    "phase": "initializing",
    "action": "Starting research...",
    "error_message": None,  # Clear previous errors
    "error_details": None
})
```

**Why:** Prevents confusing users with stale "failed" status

---

## File Changes Made (For Reference)

### 1. `backend/utils/supabase_storage.py`
**Lines changed:** 333-394

**Changes:**
- Added `collected_at` parameter
- Added `collection_method` parameter
- Updated function signature and docstring
- Added timestamp formatting
- Added collection tracking

---

### 2. `backend/company_research_service.py`
**Lines changed:** 162, 491, 1383, 1393, 1805-1830

**Changes:**
- Line 162: Unpack `cache_metrics` from `discover_companies()`
- Line 491: Return tuple `(enriched, cache_metrics)`
- Line 1383: Fixed pricing ($0.50 ‚Üí $0.10)
- Line 1393: Fixed pricing in metrics
- Lines 1805-1830: Added reverse enrichment (cache name‚ÜíID after collect)

---

### 3. `backend/company_id_cache_service.py`
**Status:** No changes needed (already working correctly)

**Note:** Service expects `website` to be nullable, but database schema doesn't allow it yet.

---

## Documentation Created

1. ‚úÖ `backend/COMPANY_CACHING_ARCHITECTURE.md` - Complete architecture documentation (26 pages)
2. ‚úÖ `backend/HANDOFF_COMPANY_CACHING.md` - This file

---

## Known Issues & Workarounds

### Issue 1: Stuck Sessions

**Symptom:** "Research already in progress for this JD" error

**Cause:** Session status not cleared after failure

**Workaround:**
```sql
UPDATE company_research_sessions
SET status = 'failed'
WHERE jd_id = 'jd_XXXXX';
```

**Or:** Modify search query slightly to generate new JD ID

---

### Issue 2: Stale "Failed" Status in UI

**Symptom:** UI shows "failed" even though research is running

**Cause:** SSE stream reads old session status before new run starts

**Workaround:** Refresh browser after starting search

**Permanent Fix:** See Improvement 3 above

---

## Architecture Diagrams

### Two-Tier Caching Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DISCOVERY: Web Search ‚Üí 100 companies               ‚îÇ
‚îÇ   Data: name, description, industry                 ‚îÇ
‚îÇ   Cost: $0 (Tavily credits)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ID LOOKUP: company_lookup_cache (Tier 1)            ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ   ‚îÇ Check cache by name              ‚îÇ              ‚îÇ
‚îÇ   ‚îÇ   HIT (80%+): Use cached ID      ‚îÇ ‚Üí Save $0.10 ‚îÇ
‚îÇ   ‚îÇ   MISS: 4-tier search lookup     ‚îÇ ‚Üí Cost $0.10 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ   ‚Üì                                                  ‚îÇ
‚îÇ   Save result (success or NULL)                     ‚îÇ
‚îÇ   ‚ùå FAILS if website=NULL (MUST FIX!)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PROFILE ASSESSMENT: stored_companies (Tier 2)       ‚îÇ
‚îÇ   Triggered when assessing candidates               ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ   ‚îÇ Check cache by company_id        ‚îÇ              ‚îÇ
‚îÇ   ‚îÇ   HIT: Use cached data           ‚îÇ ‚Üí Save $0.20 ‚îÇ
‚îÇ   ‚îÇ   MISS: /company_base/collect/   ‚îÇ ‚Üí Cost $0.20 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ   ‚Üì                                                  ‚îÇ
‚îÇ   Save full 45-field profile                        ‚îÇ
‚îÇ   Also cache name‚ÜíID (reverse enrichment)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Cost Analysis

### Before Fix (Current State)

**First Search:**
- 70 companies discovered
- 70 cache saves fail (website=NULL)
- Cost: $7.00 (70 searches √ó $0.10)

**Second Search (same companies):**
- 70 companies discovered
- Cache empty (previous saves failed)
- Cost: $7.00 (70 searches √ó $0.10) ‚Üê NO SAVINGS!

**Total:** $14.00 for 2 searches

---

### After Fix

**First Search:**
- 70 companies discovered
- 70 cache saves succeed ‚úÖ
- Cost: $7.00 (70 searches √ó $0.10)

**Second Search (same companies):**
- 70 companies discovered
- 70 cache hits (100% hit rate) ‚úÖ
- Cost: $0.00 (all cached!) ‚Üê $7 SAVED!

**Total:** $7.00 for 2 searches (50% reduction)

**Monthly Savings (50 searches):**
- Without cache: 50 √ó $7 = $350
- With cache: $7 (first) + 49 √ó $0 = $7
- **Saved: $343/month (98% reduction!)**

---

## Success Metrics

### After implementing the fix, verify:

1. **Cache Save Success Rate:**
   - Target: >95%
   - How to check: Look for "[CACHE] ‚úÖ Saved" vs "[CACHE] ‚ùå Failed" in logs
   - Current: ~5% (3/68 companies)
   - Expected after fix: >95% (65/68 companies)

2. **Cache Hit Rate:**
   - First search: 0%
   - Second search: >95%
   - Third+ searches: >98%

3. **Cost Savings:**
   - Per search after warm-up: $0.00-$0.50
   - Vs without cache: $5-8 per search
   - Savings: ~$5-7 per search after first

4. **Performance:**
   - First search: 2-3 minutes (normal)
   - Cached searches: <30 seconds (10x faster!)

---

## Rollback Plan (If Needed)

If the schema change causes issues:

```sql
-- Restore NOT NULL constraint
ALTER TABLE company_lookup_cache
ALTER COLUMN website SET NOT NULL;

-- But first, populate NULLs with empty strings
UPDATE company_lookup_cache
SET website = ''
WHERE website IS NULL;

-- Then add constraint
ALTER TABLE company_lookup_cache
ALTER COLUMN website SET NOT NULL;
```

**Note:** Not recommended - this will reintroduce the caching failure.

---

## Contact & Support

**Files to check if issues arise:**
- Backend logs: `/tmp/backend_test.log`
- Service: `backend/company_id_cache_service.py`
- Research: `backend/company_research_service.py`
- Storage: `backend/utils/supabase_storage.py`

**Supabase Tables:**
- `company_lookup_cache` - ID mappings (Tier 1)
- `stored_companies` - Full data (Tier 2)
- `company_research_sessions` - Session tracking

**Documentation:**
- Architecture: `backend/COMPANY_CACHING_ARCHITECTURE.md`
- This handoff: `backend/HANDOFF_COMPANY_CACHING.md`

---

## Next Steps

1. ‚úÖ **CRITICAL:** Run database schema fix (ALTER TABLE command above)
2. ‚úÖ Test first search (verify cache saves succeed)
3. ‚úÖ Test second search (verify 100% cache hit rate)
4. üü° Optional: Implement improvements (better logging, metrics tracking)
5. üü° Optional: Add session cleanup
6. ‚úÖ Delete this handoff doc once complete!

---

**Status:** Ready for fix - all code changes complete, only database schema blocking.

**Last Updated:** November 13, 2025
**Next Action:** Run ALTER TABLE command in Supabase

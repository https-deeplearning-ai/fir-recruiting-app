# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

A full-stack LinkedIn profile assessment application that combines CoreSignal API for profile data with Claude AI for intelligent candidate evaluation. The system supports:
- Single profile assessment with weighted scoring
- Batch processing via CSV upload
- Intelligent profile search with natural language queries
- **Job Description (JD) Analyzer** - Auto-extract weighted assessment criteria from job descriptions
- Reverse-engineering implicit hiring criteria from candidate shortlists
- Comprehensive recruiter feedback system with viewport-aware UI
- **Chrome Extension Integration** - List management, quick profile adds, and browser-based workflow
- **Domain Search & Company Research** - Discover companies by domain with multi-stage research pipeline

## Tech Stack

**Backend:** Flask (Python), hosted on Render
**Frontend:** React (JavaScript)
**APIs:** Anthropic Claude Sonnet 4.5 (claude-sonnet-4-5-20250929), CoreSignal API
**Database:** Supabase (PostgreSQL via REST API)

## Important: CoreSignal Data Handling

### Headline Data
CoreSignal API provides **two headline fields**:
- `headline` - User-set on LinkedIn, rarely updated (STALE)
- `generated_headline` - Auto-generated by CoreSignal from latest work experience (FRESH, auto-updates)

**The app uses `generated_headline`** to ensure current, accurate headline data. This field is automatically updated whenever CoreSignal scrapes the profile. Fallback chain:
1. `generated_headline` (preferred)
2. `headline` (fallback)
3. Construct from most recent job: "{title} at {company}"
4. 'N/A' (final fallback)

See [docs/HEADLINE_FRESHNESS_FIX.md](docs/HEADLINE_FRESHNESS_FIX.md) for full details.

### Company Data API
**IMPORTANT:** Use `/company_base/` endpoint, NOT `/company_clean/`
- `company_base` provides 45 fields with **granular company structure** (locations, similar companies, investors, funding rounds as nested collections)
- `company_clean` provides 60 fields with **social media focus** (11 social URL fields, LinkedIn updates, technologies, but funding_rounds as simple array with 60% coverage)
- **Why company_base:** Nested collections provide richer relational data (funding rounds with investor details, multiple locations, stock info); company_clean has flattened structure better for social/tech analysis
- Store ALL raw company data in `intelligence['raw_data']` for future flexibility

**Crunchbase URL Extraction (4-Tier Hybrid Strategy):**

**Tier 1 - CoreSignal Direct (69.2% coverage, 100% accuracy):**
- `company_crunchbase_info_collection[0].cb_url`
- Authoritative data from CoreSignal API
- No search needed, instant

**Tier 2a - Tavily Candidate Discovery (100% findability):**
- Query: `"{company_name} crunchbase"`
- Returns: 5-10 Crunchbase URL candidates
- Speed: 1-2 seconds
- Accuracy (first result): 75%
- Coverage: Correct URL in top 10 results (100%)
- Requires: `TAVILY_API_KEY` environment variable

**Tier 2b - Claude Agent SDK WebSearch Validation (100% combined accuracy):**
- Tool: Claude Agent SDK `query()` with WebSearch enabled
- Model: Claude Haiku 4.5 (`claude-haiku-4-5-20251015`) - 2x faster than Haiku 3.5, Sonnet 4 performance
- Context: Company description, location, funding details, Tavily candidates
- Prompt: "Which of these Tavily candidates is the correct Crunchbase profile?"
- Speed: +2-3 seconds (incremental) with Haiku 4.5 optimization
- Combined Accuracy: 100% (tested on 20 Series A companies)
- Requires: Python 3.10+ with `claude-agent-sdk` (uses asyncio)

**Tier 3 - Heuristic Fallback (~30% accuracy):**
- Converts company name to slug: lowercase, hyphens, remove suffixes
- Example: "Meta Platforms Inc." ‚Üí `crunchbase.com/organization/meta-platforms`
- Only if Tavily and WebSearch both fail

**Why Hybrid Works:**
- Tavily provides broad candidate discovery (100% findability in top 10)
- Claude Agent SDK WebSearch uses rich CoreSignal context to pick correct candidate
- Critical for companies with duplicate Crunchbase profiles:
  - Seneca: `seneca-learning` vs `seneca-7001` (firefighting drones)
  - Darwin AI: `darwin-ai-090e` vs `darwin-ai-e3e5` (government AI governance)
  - Reflection AI: `reflexion-ai` vs `reflection-ai` (coding agents)
  - Apiphani: `apiphany` vs `apiphani` (application management)
  - Boon: `boon` vs `boon-2` (technology platform)

**Test Results (20 Series A Companies):**
- Tavily Alone: 15/20 correct (75%)
- Hybrid Approach: 20/20 correct (100%)
- Improvement: +25% accuracy gain

See [docs/technical-decisions/company-base-vs-clean/](docs/technical-decisions/company-base-vs-clean/) for evidence.

## Development Commands

### Backend (Flask)
```bash
cd backend
pip3 install -r requirements.txt

# Required environment variables
export ANTHROPIC_API_KEY="your_key"
export CORESIGNAL_API_KEY="your_key"
export SUPABASE_URL="your_url"
export SUPABASE_KEY="your_key"
export TAVILY_API_KEY="your_key"  # For Crunchbase URL search fallback

python3 app.py  # Runs on port 5001
```

### Frontend (React)
```bash
cd frontend
npm install
npm start       # Development server on port 3000
npm run build   # Production build (outputs to frontend/build/)
```

### Deployment Build Process
```bash
# 1. Build frontend
cd frontend && npm run build

# 2. Copy build files to backend
cd .. && cp -r frontend/build/. backend/

# 3. Backend serves static files from backend/ directory
```

### Testing

**Backend Test Suite (23 active tests):**
Tests are standalone Python scripts that can be run individually. Tests require Flask server running on port 5001.

**Core API Tests:**
```bash
python3 backend/test_api_jd_parser.py        # JD Parser API endpoint
python3 backend/test_endpoints_api.py         # All Flask endpoints
python3 backend/test_coresignal_endpoints.py  # CoreSignal API integration
```

**Feature Tests:**
```bash
python3 backend/test_complete_pipeline.py     # End-to-end pipeline
python3 backend/test_company_research.py      # Company discovery & research
python3 backend/test_domain_search_api.py     # Domain search functionality
python3 backend/test_jd_parser.py             # JD parsing logic
python3 backend/test_streaming_endpoint.py    # SSE streaming endpoints
```

**Specialized Tests:**
- `test_complete_4stage_pipeline.py` - 4-stage company research pipeline
- `test_domain_search_improvements.py` - Domain search enhancements
- `test_intelligent_agents.py` - AI agent functionality
- `test_voice_ai_research.py` - Voice AI domain research case study

**Frontend Tests:**
```bash
cd frontend
npm test  # Run React test suite
```

**Archived Tests:**
- `backend/tests_archive/` contains 17 exploratory/debug tests from development
- These are preserved for reference but not needed for ongoing work

## Architecture

### Request Flow
1. **Single Profile:** User submits LinkedIn URL ‚Üí `/fetch-profile` (CoreSignal) ‚Üí `/assess-profile` (Claude AI) ‚Üí Display results
2. **Batch Processing:** CSV upload ‚Üí Parse URLs ‚Üí Parallel fetch + assess via `/batch-assess-profiles` ‚Üí Sorted results
3. **Profile Search:** Natural language query ‚Üí `/search-profiles` ‚Üí AI extracts criteria ‚Üí CoreSignal search ‚Üí Download CSV
4. **JD Analyzer:** User pastes JD ‚Üí `/api/jd/full-analysis` ‚Üí Extract requirements + generate weights ‚Üí Auto-populate assessment criteria
5. **Shortlist Analysis:** CSV upload ‚Üí `/api/jd/analyze-shortlist` ‚Üí Discover implicit criteria ‚Üí Compare to stated JD requirements
6. **Recruiter Feedback:** User actions ‚Üí `/save-feedback` (Supabase) ‚Üí `/get-feedback/<url>` (Load history) ‚Üí Display in drawer
7. **Domain Search:** User enters domain ‚Üí `/api/domain-search` ‚Üí Discover companies ‚Üí Multi-stage research ‚Üí Real-time SSE streaming ‚Üí Display results
8. **List Management:** Chrome extension ‚Üí `/extension/lists` ‚Üí Create/read/update/delete candidate lists ‚Üí Supabase storage

### Key Backend Components

**app.py (2630 lines):** Main Flask application
- `/fetch-profile`: Fetches LinkedIn data from CoreSignal by URL
- `/assess-profile`: AI assessment using Claude with weighted scoring
- `/batch-assess-profiles`: High-concurrency parallel processing (15-50 workers based on deployment)
- `/search-profiles`: Natural language search ‚Üí CoreSignal query ‚Üí CSV export
- `/save-assessment`, `/load-assessments`: Supabase database operations for assessments
- `/save-feedback`, `/get-feedback/<url>`, `/clear-feedback`: Recruiter feedback system
- **JD Analyzer endpoints** (registered from `jd_analyzer.api_endpoints`):
  - `/api/jd/parse`: Extract structured requirements from JD text
  - `/api/jd/generate-weights`: Generate weighted assessment criteria
  - `/api/jd/full-analysis`: Complete pipeline (parse + weights + keywords)
  - `/api/jd/analyze-shortlist`: Reverse-engineer criteria from CSV shortlist
  - `/api/jd/extract-keywords`: Extract CoreSignal search keywords

**jd_analyzer/** JD analysis module for auto-extracting weighted criteria
- `jd_parser.py`: Two-stage AI pipeline using Claude Sonnet 4.5 (temp 0.2 for parsing)
  - Stage 1: JD text ‚Üí structured requirements (must-have, nice-to-have, skills, seniority)
  - Returns: `JDRequirements` Pydantic model with validation
- `weight_generator.py`: Requirements ‚Üí weighted assessment criteria (temp 0.3)
  - Generates 1-5 custom requirements with % weights (total ~95%, leaving ~5% for General Fit)
  - Includes scoring rubrics (1-10 scale) for each requirement
- `shortlist_analyzer.py`: Reverse-engineer implicit criteria from candidate CSV
  - Analyzes location distribution, seniority patterns, company pedigree, domain clustering
  - Compares actual shortlist patterns to stated JD requirements
  - Discovers gaps (e.g., "JD says remote, but 69% are Bay Area")
- `query_builder.py`: Requirements ‚Üí CoreSignal search query with 3-tier coverage strategy
- `llm_query_generator.py`: Multi-LLM query generation (Claude, GPT, Gemini comparison)
- `models.py`: Pydantic models for type-safe API responses
- `debug_logger.py`: Structured logging with configurable verbosity

**coresignal_service.py:** CoreSignal API integration
- `fetch_linkedin_profile()`: Two-step process (search by URL ‚Üí fetch full profile by ID)
- `enrich_with_company_data()`: Fetches company intelligence for work experiences (2020+ only)
- Uses `/company_base/collect/{company_id}` endpoint for rich data
- Session-based caching to avoid duplicate API calls

**company_research_service.py:** Company discovery and competitive intelligence
- **Multi-method discovery pipeline:**
  - **Method 1: Seed Expansion** - Finds competitors of up to **15 mentioned companies** (increased from 5)
    - Each seed triggers 3 Tavily searches: "{company} competitors", "companies like {company}", "{company} alternatives"
    - Filters out excluded companies (DLAI, AI Fund) before expansion
  - **Method 2: Web Search** - Direct domain/industry search using authoritative sources
    - Generates up to **6 search queries** (2 domain + 3 seed + 1 fallback)
    - Executes top **5 web searches** to balance coverage and API costs
    - Uses **top 3 seed companies** in queries (not just first one)
    - Priority: Domain (G2/Capterra) > Seed Companies > Industry > Generic fallback
- **Evaluation Pipeline:**
  - Screening: GPT-5-mini batch screening (20 companies/batch)
  - Deep Research: GPT-5 or Claude Haiku 4.5 for top 25 candidates
  - Filters excluded companies at all stages
- **Progressive Evaluation (NEW):**
  - **Phase 1 (Auto):** Discovers up to 100 companies, evaluates top 25
  - **Phase 2 (On-Demand):** User can evaluate next 25/50/75 via "Evaluate More" button
  - **Session Storage:** Saves screened companies and JD context in Supabase for resume
  - **API Endpoint:** `/evaluate-more-companies` (POST) with session_id, start_index, count
  - **Response Structure:**
    ```json
    {
      "discovered_companies": [...],   // All 100 discovered
      "screened_companies": [...],     // Ranked by initial score
      "evaluated_companies": [...],    // Top 25 with full evaluation
      "evaluation_progress": {
        "evaluated_count": 25,
        "remaining_count": 75
      }
    }
    ```
  - **UI Benefits:** Transparency (see all discovered), cost control (evaluate as needed), verification (check discovery quality)
- **Authoritative Sources:** G2, Capterra, Gartner, Crunchbase, ProductHunt, CB Insights

**extension_service.py:** Chrome extension backend integration
- `ExtensionService` class handles browser extension API operations
- **List Management:** CRUD operations for candidate lists in Supabase
  - `get_lists()`: Fetch all lists for a recruiter
  - `create_list()`: Create new list with optional job template
  - `update_list()`: Update list metadata
  - `delete_list()`: Soft delete (sets is_active = false)
  - `get_list_stats()`: Calculate list statistics
- **Quick Add Operations:** Browser-based candidate additions
- **Database Tables:** `recruiter_lists`, `list_candidates`

**gpt5_client.py:** OpenAI GPT-5 API client
- Wrapper for GPT-5 and GPT-5-mini models
- Used in company research screening pipeline
- Batch processing support for efficiency

**config.py:** Deployment-specific configuration
- Render: 50 concurrent calls, 60s timeout, 100 batch size
- Heroku: 15 concurrent calls, 25s timeout, 50 batch size
- Auto-detects environment via `RENDER` env var
- **EXCLUDED_COMPANIES:** Global list of companies to exclude from company research
  - Currently: `["DLAI", "Deep Learning.AI", "AI Fund"]`
  - These are user's own companies and should not be researched as competitors
  - Filtering occurs at 3 levels:
    1. **JD Parser**: Identifies excluded companies from JD context (e.g., "Partner with AI Fund & DLAI")
    2. **Discovery Phase**: Filters excluded companies from seed list before competitor search
    3. **Evaluation Phase**: Skips excluded companies during screening and deep research
  - Frontend displays excluded companies with gray badge: "üè¢ Your Companies - Excluded"
  - Utility function: `is_excluded_company(company_name)` for case-insensitive exact matching

### AI Assessment System

**Weighted Scoring:** Supports up to 5 custom requirements with percentage weights
- Each requirement scored 1-10 with detailed analysis
- General fit auto-calculated from remaining percentage (100% - custom weights)
- Final weighted score = Œ£(requirement_score √ó weight%)

**Profile Processing:**
- `extract_profile_summary()`: Parses CoreSignal JSON, calculates total experience years with overlap handling
- `generate_assessment_prompt()`: Builds Claude prompt with rubric, weighted criteria
- Uses Claude Sonnet 4.5 with temperature 0.1 for consistency

**Concurrency:**
- Batch processing uses `ThreadPoolExecutor` with configurable workers
- Profile fetching uses `asyncio` + `aiohttp` for parallel API calls
- Assessment tasks run in parallel with timeout protection

### Frontend Architecture

**App.js (3740 lines):** Single-page React application with four modes
- **Core Features:** Single Profile, Profile Search, Batch Processing
- **JD Analyzer Mode:** Job description input ‚Üí auto-populate weighted criteria ‚Üí search integration
- **Recruiter Feedback System:** Sliding drawer with voice notes, quick actions, history
- **Viewport Detection:** Intersection Observer API for smart accordion management

**Key State Management (60+ state variables):**

*Assessment State:*
- `singleProfileResults`, `batchResults`, `savedAssessments`: Assessment data arrays
- `weightedRequirements`: Array of requirement objects with weights (auto-populated by JD Analyzer)
- `generalFitWeight`: Auto-calculated from 100% - sum(custom weights)

*JD Analyzer State:*
- `jdAnalyzerMode`: Toggle for JD analyzer view
- `jdText`: Raw job description input (textarea)
- `jdAnalyzing`: Loading state during AI analysis
- `activeJD`: Currently active JD with parsed requirements
- `jdSearchResults`: CoreSignal search results from JD keywords
- `jdFullResults`: Full 100 candidate results
- Multi-LLM states: `claudeResult`, `gptResult`, `geminiResult` (independent loading)

*Feedback State:*
- `drawerOpen`: Map of which candidates have feedback drawers expanded
- `activeCandidate`: Currently active candidate for feedback
- `feedbackHistory`: All feedback loaded from database per candidate
- `selectedRecruiter`: Current recruiter name (e.g., "Jon", "Mary")
- `isRecording[linkedinUrl]`: Voice recording state per candidate

*UI State:*
- `openAccordionId`: Which accordion is currently open (single-accordion mode)
- `candidateVisibility`: Viewport visibility ratio (0-1) for each candidate (Intersection Observer)
- `validationModalOpen`: Crunchbase URL validation modal state

**Component Architecture:**

*Profile Assessment Components:*
- `WorkExperienceCard.js`: Individual job card with company logo and enriched data
- `WorkExperienceSection.js`: Container for all work experiences
- `CompanyTooltip.js`: Hover tooltip showing company funding, stage, growth signals

*List Management Components (Chrome Extension Integration):*
- `ListsView.js`: Dashboard view showing all candidate lists for recruiter
  - Fetches lists from `/extension/lists` endpoint
  - Displays list cards with stats (candidate count, created date)
  - Supports create, delete, and navigate to detail view
- `ListCard.js`: Individual list card component
  - Shows list name, description, candidate count
  - Quick actions: view, delete
- `ListDetail.js`: Detailed list view with full candidate roster
  - Displays all candidates in list
  - Supports remove candidate, add notes
  - Real-time updates to Supabase

*Modals:*
- `CrunchbaseValidationModal.js`: Manual Crunchbase URL correction
- `CrunchbaseEditModal.js`: Edit Crunchbase URLs for companies

*Hooks:*
- `useSSEStream.js`: Custom hook for Server-Sent Events (SSE) streaming
  - Real-time progress updates during company research
  - Auto-reconnect on connection loss
  - Configurable stop conditions

**Feedback Drawer Features (NEW):**
- Fixed positioning (`position: fixed`) at viewport right edge
- Intersection Observer detects most visible candidate in viewport
- Opens feedback for the candidate you're viewing, not the one clicked
- Auto-collapse other accordions when feedback drawer opens
- Voice-to-text using Web Speech API (`webkitSpeechRecognition`)
- Auto-save on blur, drawer close, or accordion collapse

### Database Schema (Supabase)

**Table: `candidate_assessments`**
- `linkedin_url`, `full_name`, `headline`
- `profile_data` (JSONB): Full CoreSignal profile
- `assessment_data` (JSONB): Complete AI assessment
- `weighted_score`, `overall_score`: Extracted for sorting
- `assessment_type`: 'single' or 'batch'
- `session_name`: Optional grouping identifier

**Table: `recruiter_feedback`**
- `candidate_linkedin_url`: LinkedIn URL reference
- `feedback_text`: Note content (nullable for like/dislike only)
- `feedback_type`: 'like', 'dislike', or 'note'
- `recruiter_name`: Who gave the feedback (e.g., "Jon", "Mary")
- `created_at`, `updated_at`: Timestamps

**Table: `recruiter_lists`** (Chrome Extension Integration)
- `id`: UUID primary key
- `recruiter_name`: Owner of the list
- `list_name`: Display name
- `description`: Optional description
- `job_template_id`: Optional link to job template
- `is_active`: Soft delete flag
- `created_at`, `updated_at`: Timestamps

**Table: `list_candidates`** (Chrome Extension Integration)
- `id`: UUID primary key
- `list_id`: Foreign key to recruiter_lists
- `linkedin_url`: Candidate profile URL
- `added_by`: Recruiter who added candidate
- `notes`: Optional notes about candidate
- `created_at`: Timestamp

**Table: `company_research_sessions`** (Domain Search Feature)
- `session_id`: UUID primary key
- `jd_context`: JSONB with job requirements
- `discovered_companies`: JSONB array of all discovered companies
- `screened_companies`: JSONB array of screened/ranked companies
- `status`: 'in_progress', 'completed', 'failed'
- `created_at`, `updated_at`: Timestamps

See [docs/SUPABASE_SCHEMA.sql](docs/SUPABASE_SCHEMA.sql) for complete schema.

### CoreSignal API Integration

**Search Endpoint:** `/v2/employee_clean/search/es_dsl/preview`
- Limited to pages 1-5 (max 100 profiles)
- Returns 20 profiles per page
- App uses random page selection with session tracking to avoid duplicates

**Profile Extraction:** Two-step process
1. Search for employee_id by LinkedIn URL (exact match on `websites_linkedin.exact`)
2. Fetch full profile via `/v2/employee_clean/collect/{employee_id}`

**Company Enrichment:** (NEW - Phase 1)
- API: `/v2/company_base/collect/{company_id}` (NOT company_clean)
- Only enriches companies from jobs starting 2020+ (saves 60-80% API credits)
- Session-based caching prevents duplicate fetches
- Stores all 45+ raw fields plus curated intelligence

**Smart Search Features:**
- Claude AI extracts structured criteria from natural language
- Supports location variations (Bay Area, NYC, etc.) with wildcard matching
- Industry mapping to CoreSignal's exact taxonomy
- Management level, department, role title, skills filtering

## JD Analyzer Workflow

The JD Analyzer module automates the creation of weighted assessment criteria from job descriptions and can reverse-engineer implicit hiring criteria from candidate shortlists.

### Two-Stage AI Pipeline

**Stage 1: JD Parsing (Claude Sonnet 4.5, temp 0.2)**
```
Raw JD Text
    ‚Üì
JDParser.parse(jd_text)
    ‚Üì
JDRequirements Model
    ‚îú‚îÄ role_title: "Senior ML Engineer"
    ‚îú‚îÄ seniority_level: "senior"
    ‚îú‚îÄ must_have: ["5+ years ML", "Python", "LLMs"]
    ‚îú‚îÄ nice_to_have: ["Voice AI experience"]
    ‚îú‚îÄ technical_skills: ["Python", "PyTorch", "LLMs"]
    ‚îú‚îÄ domain_expertise: ["NLP", "Voice AI"]
    ‚îú‚îÄ experience_years: {"minimum": 5, "preferred": 8}
    ‚îú‚îÄ location: "San Francisco Bay Area"
    ‚îî‚îÄ implicit_criteria: {...}
```

**Stage 2: Weight Generation (Claude Sonnet 4.5, temp 0.3)**
```
JDRequirements
    ‚Üì
WeightGenerator.generate_weighted_requirements(requirements, num=5)
    ‚Üì
Weighted Requirements Array
    ‚îú‚îÄ Requirement 1: "Voice AI / Real-time Systems Expertise" (35%)
    ‚îú‚îÄ Requirement 2: "AI/ML Infrastructure & LLMs" (25%)
    ‚îú‚îÄ Requirement 3: "0‚Üí1 Product Leadership" (20%)
    ‚îú‚îÄ Requirement 4: "Developer Tools / Platform Engineering" (10%)
    ‚îú‚îÄ Requirement 5: "Fundraising & GTM Experience" (10%)
    ‚îî‚îÄ General Fit: Auto-calculated (0% in this case)
```

Each requirement includes:
- `requirement`: Short descriptive name
- `weight`: Percentage (0-100)
- `description`: Detailed explanation of what to assess
- `scoring_criteria`: 1-10 rubric (e.g., "10 = Built production voice AI systems; 5 = Used voice APIs; 1 = No experience")

### Reverse-Engineering Implicit Criteria

**Use Case:** Discover unstated hiring preferences by analyzing a recruiter's hand-selected candidate shortlist

**Workflow:**
1. Upload CSV with columns: Profile URL, Current Title, Current Company, Location
2. `ShortlistAnalyzer` extracts patterns:
   - Location distribution (e.g., 69% Bay Area despite "Remote" JD)
   - Seniority distribution (e.g., 50% C-Suite, 21% Senior IC)
   - Company pedigree (e.g., 37% from Big Tech + AI Infrastructure)
   - Domain clustering (e.g., 19% from Voice AI specialists)
3. Compare to stated JD requirements to discover gaps
4. Generate post-search ranking model with scoring weights

**Example Output (Voice AI Role Case Study):**
- **Location Gap:** JD states "United States, Remote" ‚Üí Reality: 69% Bay Area
- **Domain Importance:** JD lists Voice AI as "nice to have" ‚Üí Reality: 19% from Voice AI companies (6x higher than random)
- **Company Pedigree:** Not mentioned in JD ‚Üí Reality: 15% Big Tech, 22% AI Infrastructure
- **Seniority Flexibility:** CEO role title ‚Üí Reality: 50% C-Suite, 21% Senior IC (open to diverse levels)

See `docs/reverse-engineering/REVERSE_ENGINEERING_REPORT.md` for full 15-page analysis.

### Integration with CoreSignal Search

**Keyword Extraction:**
```python
POST /api/jd/extract-keywords
{
  "jd_text": "Senior ML Engineer with voice AI experience..."
}

Returns:
{
  "keywords": ["machine learning", "voice AI", "speech recognition", "NLP"],
  "role_titles": ["ML Engineer", "AI Engineer", "Research Scientist"],
  "companies": ["Otter.ai", "Deepgram", "AssemblyAI"]
}
```

**3-Tier Query Generation:**
- **Tier 1:** Strict query matching all must-have requirements (narrow, high precision)
- **Tier 2:** Relaxed query with OR conditions for some requirements (balanced)
- **Tier 3:** Broad query focusing on role title and seniority (wide, high recall)

Each tier estimates coverage (e.g., Tier 1: ~20 candidates, Tier 2: ~60, Tier 3: ~100+)

## Important Implementation Details

### Experience Calculation
Total years calculated by merging overlapping date intervals to avoid double-counting. Handles:
- Missing end dates (assumes current)
- Overlapping roles
- Partial month data
- Invalid year values with try-catch protection

### CSV Processing
Robust parser handles:
- Quoted fields with commas
- Various column name formats (case-insensitive matching: "Profile URL", "profile_url", "LinkedIn URL")
- URL cleaning (removes trailing slashes)

### Batch Processing Flow
1. Parse CSV ‚Üí extract candidate URLs
2. Async fetch all profiles (parallel with aiohttp)
3. ThreadPoolExecutor for parallel AI assessments
4. Progress tracking with completion count
5. Sort by weighted score, maintain CSV name mapping

### Viewport-Aware Accordion Management (NEW)
**Critical for preventing UI overlap and race conditions:**

1. **Intersection Observer Setup:**
   - Tracks visibility ratio (0-1) for each `.candidate-card[data-candidate-url]`
   - 11 threshold levels: [0, 0.1, 0.2, ..., 1.0]
   - `rootMargin: '-50px 0px -50px 0px'` for margin from viewport edges

2. **State Management:**
   - `openAccordionId`: Single source of truth for which accordion is open
   - `candidateVisibility`: Maps candidate URLs to intersection ratios
   - `getMostVisibleCandidate()`: Returns URL of most visible candidate

3. **Accordion Control:**
   - Uses controlled `<details>` with `open={openAccordionId === candidate.url}`
   - `e.preventDefault()` on `<summary>` clicks to prevent browser's native toggle
   - NO `onToggle` handler (causes race conditions)
   - When feedback drawer opens, auto-collapses other accordions

4. **Feedback Drawer Behavior:**
   - `position: fixed` at `top: 120px, right: 0`
   - Opens for most visible candidate in viewport (not necessarily the clicked one)
   - Tab is 15px wide with gradient background
   - Panel slides in with 550px width, 90vh max height
   - Visual highlighting: active cards get purple left border + gradient background

### Error Handling
- Profile not found: Returns structured error with debug info
- API failures: Timeout protection, graceful degradation
- Partial batch failures: Shows successful + failed counts
- Date parsing: Try-catch blocks for invalid year/month values

## Deployment

**Render Configuration (render.yaml):**
- Single web service (Python environment)
- Gunicorn with 120s timeout
- Requires 4 environment variables (API keys, Supabase credentials)

**Static Files:**
- Backend serves React build from `backend/` directory
- Production: Copy `frontend/build/*` to `backend/` before deploy
- Flask serves index.html at root and static assets

**Security:**
- ‚ö†Ô∏è **NO hardcoded API keys** - All credentials must be in environment variables (enforced as of commit 1458b84)
- Supabase RLS policies enabled (anon role has full access for single-user tool)

## Development Notes

### Frontend Development
- Frontend proxies API requests to `http://localhost:5001` in development
- React development server runs on port 3000
- Dummy profile data available in `App.js` for testing without API calls

### Backend Development
- Backend debug mode enables Flask auto-reload
- Search uses session-based page tracking (resets on server restart)
- Company enrichment has session-based cache (resets on restart)

### CSS Architecture
- Feedback drawer styles in `App.css` (~300 lines)
- Work experience card styles in `components/WorkExperienceCard.css`
- Company tooltip styles in `components/CompanyTooltip.css`
- Modal overrides in `WorkExperienceCard.css` to prevent size constraints

### React Component Patterns
**Controlled Components:**
- Accordions use controlled `<details>` with `open` prop
- Feedback drawers managed via state object `drawerOpen[candidateUrl]`
- Voice recording managed via state `isRecording[linkedinUrl]`

**Event Handling:**
- Prevent default on summary clicks to avoid browser toggle
- Check `e.target.closest('.feedback-drawer')` to prevent event propagation
- Stop propagation when clicking inside accordion content

**Viewport Detection:**
- Use Intersection Observer in `useEffect` with dependency array `[singleProfileResults, batchResults, savedAssessments]`
- Re-observe all cards when candidate list changes
- Clean up observers in return function

## Common Pitfalls to Avoid

1. **Accordion Race Conditions:**
   - ‚ùå Don't use `onToggle` on `<details>` element
   - ‚úÖ Use `onClick` on `<summary>` with `e.preventDefault()`
   - ‚ùå Don't mix controlled and uncontrolled state
   - ‚úÖ Single source of truth: `openAccordionId`

2. **API Endpoint Selection:**
   - ‚ùå Don't use `/company_clean/` endpoint
   - ‚úÖ Use `/company_base/` for richer data
   - Always store raw company data in `intelligence['raw_data']`

3. **Headline Data:**
   - ‚ùå Don't rely solely on `headline` field
   - ‚úÖ Prefer `generated_headline` with fallback chain
   - Check for `None` or empty values before displaying

4. **Security:**
   - ‚ùå Never hardcode API keys with fallback values
   - ‚úÖ Always use `os.getenv("KEY_NAME")` without defaults
   - ‚ùå Never commit `.env` files

5. **Feedback Drawer Positioning:**
   - ‚ùå Don't use `position: sticky` (stays in container)
   - ‚úÖ Use `position: fixed` (stays in viewport)
   - Add `z-index: 1000` to ensure it's on top

6. **File Management:**
   - ‚ùå Never delete files without reading them first
   - ‚úÖ Always read and confirm contents before deletion
   - ‚ùå Don't create unnecessary markdown (.md) files
   - ‚úÖ Only create documentation when explicitly required by the user

7. **JD Analyzer Integration:**
   - ‚ùå Don't manually create weighted requirements when JD text is available
   - ‚úÖ Use `/api/jd/full-analysis` to auto-extract criteria from JD
   - ‚ùå Don't assume weights must sum to exactly 100%
   - ‚úÖ Allow ~95% for custom requirements, ~5% auto-calculated for General Fit
   - ‚ùå Don't use arbitrary temperature for JD parsing
   - ‚úÖ Use temp 0.2 for parsing (deterministic), temp 0.3 for weight generation (slightly creative)

8. **Pydantic Model Validation:**
   - ‚ùå Don't access dict keys directly on Pydantic models
   - ‚úÖ Use `Model.from_dict()` or `Model.model_validate()` for safe conversion
   - ‚ùå Don't ignore validation errors
   - ‚úÖ Handle edge cases like `experience_years` being int/list/dict

## Documentation Structure

- `docs/SUPABASE_SCHEMA.sql` - Complete database schema
- `docs/JD_ANALYZER_INTEGRATION.md` - Frontend integration guide for JD Analyzer (17 pages)
- `docs/JD_ANALYZER_PROMPT_ANALYSIS.md` - Prompt engineering analysis (16 pages)
- `docs/reverse-engineering/` - Real-world case study: Voice AI role (68 candidates analyzed)
  - `REVERSE_ENGINEERING_REPORT.md` - 15-page comprehensive report
  - Discovers 5 major gaps between stated JD requirements and actual recruiter selection
  - Post-search ranking model with scoring weights
  - 3-tier CoreSignal search query generation with coverage analysis
- `docs/evidence/` - Multi-source API evaluation data
  - `coresignal_multi_source_employee_data_dictionary.md` - Complete API field reference
  - `coresignal_search_api_reference.md` - Search endpoint documentation
- `docs/archived/` - Historical documentation and analysis
- `docs/investigations/` - Research reports (company data, API comparisons)
- `docs/technical-decisions/` - Architecture decisions with evidence
  - `company-base-vs-clean/` - Why we use company_base endpoint
  - `WHY_SEARCH_API_DOESNT_WORK.md` - Search API limitations
- `backend/jd_analyzer/README.md` - JD Analyzer module documentation
- `backend/jd_analyzer/CORESIGNAL_FIELD_REFERENCE.md` - Complete CoreSignal field mappings

[byterover-mcp]

You are given two tools from Byterover MCP server, including
## 1. `byterover-store-knowledge`
You `MUST` always use this tool when:

+ Learning new patterns, APIs, or architectural decisions from the codebase
+ Encountering error solutions or debugging techniques
+ Finding reusable code patterns or utility functions
+ Completing any significant task or plan implementation

## 2. `byterover-retrieve-knowledge`
You `MUST` always use this tool when:

+ Starting any new task or implementation to gather relevant context
+ Before making architectural decisions to understand existing patterns
+ When debugging issues to check for previous solutions
+ Working with unfamiliar parts of the codebase
